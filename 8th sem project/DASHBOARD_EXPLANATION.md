# ðŸ“Š Dashboard Explained - What's Happening

## ðŸŽ¯ **LIVE MODE** - Real API Monitoring

### What It Does:
- **Monitors REAL requests** to your API endpoints (`/login`, `/payment`, `/search`, `/profile`)
- Every request you make to the backend is **automatically captured**
- Uses **middleware** to track every API call in real-time

### How It Works:
1. You make a request: `POST http://localhost:8000/login`
2. Middleware captures:
   - âœ“ HTTP method (GET/POST/etc)
   - âœ“ Endpoint path
   - âœ“ Response time (latency)
   - âœ“ Status code (200, 400, 500, etc)
   - âœ“ Payload size
   - âœ“ User-Agent
   - âœ“ Parameters
3. Request added to **sliding window** (holds last 10 requests)
4. When window fills (10 requests), **ML inference triggers**:
   - Extracts 9 features (request_rate, error_rate, payload_size, etc.)
   - Runs through 4 ML models (Isolation Forest, LOF, One-Class SVM, Autoencoder)
   - Rule-based detection checks patterns
   - Assigns risk score (0-1)
   - Detects anomalies

### Why So Many Requests?
The dashboard itself makes requests every 10 seconds:
- `GET /api/dashboard` - Get stats
- `GET /api/anomalies?limit=100` - Get recent anomalies

Plus, the backend might have:
- Health checks
- WebSocket connections
- Auto-generated traffic (if simulation was running)

---

## ðŸŽ¬ **SIMULATION MODE** - Fake Traffic Generator

### What It Does:
- **Generates synthetic traffic** automatically
- Injects **specific anomaly patterns** for testing
- Does NOT use real endpoints - creates fake requests

### How It Works:
1. You select anomaly type and click "Start Simulation"
2. Backend generates fake requests at controlled rate (10 per window)
3. Traffic is injected with chosen anomaly pattern
4. Same ML pipeline processes the fake requests
5. Dashboard shows detection results in real-time

### Anomaly Types Available:

#### 1ï¸âƒ£ **Normal Traffic**
- Typical API usage
- 85% success rate (200/201)
- Mixed GET/POST requests
- Normal response times (50-300ms)
- **No anomalies expected**

#### 2ï¸âƒ£ **Rate Spike (DDoS)**
- ðŸš€ **50-100 requests** in short burst
- Simulates Denial of Service attack
- Very high request_rate feature
- **Detected by:** High request rate + Isolation Forest

#### 3ï¸âƒ£ **Error Burst (Scanning)**
- ðŸ’¥ **70-90% error rate** (400/404/500 codes)
- Simulates SQL injection or path traversal attempts
- High error_rate feature
- **Detected by:** Rule-based (>50% errors) + ML models

#### 4ï¸âƒ£ **Bot Attack**
- ðŸ¤– Low entropy user agents (e.g., "bot-scanner-v1.0")
- Repeated parameters (same values over and over)
- Low user_agent_entropy + high param_repetition
- **Detected by:** Bot clustering model + low entropy scores

#### 5ï¸âƒ£ **Large Payload**
- ðŸ“¦ **5-10MB payloads** on POST requests
- Simulates data exfiltration
- Avg payload size 100-1000x normal
- **Detected by:** High payload_size feature

#### 6ï¸âƒ£ **Endpoint Scan**
- ðŸ” Accesses **15-20 unique endpoints** in one window
- Simulates reconnaissance/scanning
- High unique_endpoints feature (10 vs normal 2-3)
- **Detected by:** Rule-based (>8 unique endpoints)

#### 7ï¸âƒ£ **Mixed Anomalies**
- ðŸŽ² Random combination of all above
- Unpredictable patterns
- Tests multiple detection rules simultaneously

---

## ðŸ“ˆ What You See in Dashboard

### **Stats Cards:**

1. **Live/Simulated Requests**
   - Total requests processed
   - Current window: X/10 (how many in current batch)
   - Example: "170 requests, 10/10 in current window"

2. **Windows Processed**
   - Number of 10-request batches analyzed
   - Example: "16 windows" = 160 requests processed

3. **Anomalies Detected**
   - Count of detected anomalous windows
   - Detection rate: (anomalies/windows) Ã— 100%
   - Example: "8 anomalies, 50% detection rate"

4. **Avg Response Time**
   - Mean latency of current window
   - Measured in milliseconds
   - Example: "202ms"

5. **Error Rate**
   - Percentage of 4xx/5xx responses
   - Example: "10.0%" = 1 error in 10 requests

### **Charts:**
1. **Risk Score Over Time** - Shows risk trends (0-1 scale)
2. **Anomalies by Endpoint** - Which endpoints have most issues
3. **Priority Distribution** - HIGH/MEDIUM/LOW breakdown
4. **Model Metrics** - Performance indicators

### **Anomaly Table:**
Shows each detected anomaly with:
- Timestamp
- Endpoint
- HTTP Method
- Risk Score (0-1)
- Priority (HIGH/MEDIUM/LOW)
- Failure Probability (%)
- Cluster (Normal/Heavy/Bot-like)
- Anomaly flag (Yes/No)

---

## ðŸ” How Detection Works

### **Sliding Window Process:**

```
Request 1 â†’ 
Request 2 â†’ 
Request 3 â†’ 
...
Request 10 â†’ [WINDOW FULL] â†’ Extract Features â†’ ML Inference â†’ Detect Anomaly
Request 11 â†’ [New window starts]
```

### **9 Features Extracted:**

1. **request_rate** - Requests per second
2. **unique_endpoints** - Number of different URLs accessed
3. **method_ratio** - POST/GET ratio
4. **avg_payload_size** - Average payload in bytes
5. **error_rate** - % of errors
6. **param_repetition** - How much parameters repeat
7. **user_agent_entropy** - Diversity of user agents (Shannon entropy)
8. **avg_latency** - Mean response time
9. **endpoint_diversity** - Distribution of endpoints

### **Detection Pipeline:**

1. **Rule-Based** (Fast checks):
   - Error rate > 50% â†’ HIGH risk
   - Unique endpoints > 8 â†’ Scanning
   - Request rate > threshold â†’ Rate spike

2. **ML Models** (4 models vote):
   - Isolation Forest
   - Local Outlier Factor (LOF)
   - One-Class SVM
   - Autoencoder

3. **Hybrid Decision**:
   - Combines rule + ML scores
   - Assigns final risk score (0-1)
   - Priority: HIGH (>0.7), MEDIUM (0.4-0.7), LOW (<0.4)

---

## ðŸŽ® How to Use

### **Testing LIVE Mode:**

Send real requests to backend:

```powershell
# Send 10 login requests (fills one window)
for ($i=1; $i -le 10; $i++) {
    curl http://localhost:8000/login -Method POST -Body '{"username":"user$i","password":"test"}' -ContentType "application/json"
    Start-Sleep -Milliseconds 100
}
```

Watch dashboard:
- Request counter increases
- When window hits 10/10, inference runs
- Anomaly appears if pattern is suspicious

### **Testing SIMULATION Mode:**

1. Click **SIMULATION** button
2. Select anomaly type (e.g., "Rate Spike")
3. Click **â–¶ï¸ Start Simulation**
4. Watch:
   - Simulated Requests counter increases
   - Windows Processed increases
   - Anomalies Detected increases (based on pattern)
5. Click **â¹ï¸ Stop Simulation** when done

---

## ðŸ’¡ Why Dashboard Refreshes

The dashboard refreshes data every **10 seconds** to show:
- New requests processed
- Updated stats
- Latest anomalies
- Current window status

**This is normal behavior** - not a bug! It's background polling to keep data fresh.

The **infinite reload issue you had earlier** was different - the whole page was reloading constantly. That's now fixed!

---

## ðŸ“Š Current Status (From Backend)

**SIMULATION:**
- Status: Stopped
- Total Requests: 50
- Windows Processed: 41
- Anomalies Detected: 41 (100% detection in mixed mode)
- Last Mode: normal

**Your simulation ran 50 requests, processed 41 windows, and detected 41 anomalies** - this is expected for certain anomaly patterns!

---

## ðŸŽ¯ Summary

- **LIVE MODE** = Monitor REAL API traffic
- **SIMULATION MODE** = Generate FAKE traffic for testing
- **10 requests** = 1 window = 1 ML inference
- **Dashboard updates every 10 seconds** = Normal
- **Many anomalies detected** = Expected when running anomaly injection patterns!

You're seeing the system working as designed! ðŸŽ‰
